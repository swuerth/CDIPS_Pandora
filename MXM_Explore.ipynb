{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exploratory Data Analysis on the MXM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm #for colormapping later\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfile = './mxm_data/mxm_dataset_train.txt'\n",
    "testfile = './mxm_data/mxm_dataset_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header Info for Training Set:\n",
      "# TRAINING SET\n",
      "\n",
      "# MusiXmatch dataset, the official lyrics dataset\n",
      "\n",
      "# of the Million Song Dataset\n",
      "\n",
      "#    file created on Tue Mar 29 04:28:44 2011\n",
      "\n",
      "#    contact: T. Bertin-Mahieux (Columbia University)\n",
      "\n",
      "#             tb2332@columbia.edu\n",
      "\n",
      "#    also: http://labrosa.ee.columbia.edu/millionsong/musixmatch\n",
      "\n",
      "#          http://www.musixmatch.com\n",
      "\n",
      "# FORMAT:\n",
      "\n",
      "#     #   - comment, to ignore\n",
      "\n",
      "#     %   - list of top words, comma-separated\n",
      "\n",
      "#         - normal line, contains track_id, mxm track id,\n",
      "\n",
      "#           then word count for each of the top words, comma-separated\n",
      "\n",
      "#           word count is in sparse format -> ...,<word idx>:<cnt>,...\n",
      "\n",
      "#           <word idx> starts at 1 (not zero!)\n",
      "\n",
      "# All our work is done using UTF-8 encoding.\n",
      "\n",
      "# enjoy!\n",
      "\n",
      "Header Info for Testing Set:\n",
      "# TESTING SET\n",
      "\n",
      "# MusiXmatch dataset, the official lyrics dataset\n",
      "\n",
      "# of the Million Song Dataset\n",
      "\n",
      "#    file created on Tue Mar 29 04:28:44 2011\n",
      "\n",
      "#    contact: T. Bertin-Mahieux (Columbia University)\n",
      "\n",
      "#             tb2332@columbia.edu\n",
      "\n",
      "#    also: http://labrosa.ee.columbia.edu/millionsong/musixmatch\n",
      "\n",
      "#          http://www.musixmatch.com\n",
      "\n",
      "# FORMAT:\n",
      "\n",
      "#     #   - comment, to ignore\n",
      "\n",
      "#     %   - list of top words, comma-separated\n",
      "\n",
      "#         - normal line, contains track_id, mxm track id,\n",
      "\n",
      "#           then word count for each of the top words, comma-separated\n",
      "\n",
      "#           word count is in sparse format -> ...,<word idx>:<cnt>,...\n",
      "\n",
      "#           <word idx> starts at 1 (not zero!)\n",
      "\n",
      "# All our work is done using UTF-8 encoding.\n",
      "\n",
      "# enjoy!\n",
      "\n",
      "First few words of train top words : \n",
      "%i,the,you,to,and,a,me,it,not,in,my,is,of,your,that,do,on,are,we,am,will,all,for,no,be,have,love,so,\n",
      "Should be i,the,you,to...\n",
      "First few words of test top words : \n",
      "%i,the,you,to,and,a,me,it,not,in,my,is,of,your,that,do,on,are,we,am,will,all,for,no,be,have,love,so,\n",
      "Should be i,the,you,to...\n",
      "There are  210519  songs in the train set\n",
      "There are  27143  songs in the test set\n"
     ]
    }
   ],
   "source": [
    "#Header info is in the first 17 lines for the train file (and same for test file)\n",
    "ftrain = open(trainfile, 'r')\n",
    "ftest = open(testfile,'r')\n",
    "print('Header Info for Training Set:')\n",
    "for i in range(17):\n",
    "    print(ftrain.readline())\n",
    "#Header info is in the first 17 lines for the test file as well\n",
    "print('Header Info for Testing Set:')\n",
    "for i in range(17):\n",
    "    print(ftest.readline())\n",
    "    \n",
    "#line 18 for each one is the top words\n",
    "train_top_words = ftrain.readline()\n",
    "test_top_words = ftest.readline()\n",
    "print('First few words of train top words : ')\n",
    "print(train_top_words[0:100])\n",
    "print('Should be i,the,you,to...')\n",
    "\n",
    "print('First few words of test top words : ')\n",
    "print(test_top_words[0:100])\n",
    "print('Should be i,the,you,to...')  \n",
    "\n",
    "#subsequent lines in the test and train sets should be \"normal lines\" \n",
    "#as described by the header text.\n",
    "#read those in!\n",
    "test = ftest.readlines() #this will pick up where we left off and read the remaining lines\n",
    "train = ftrain.readlines() \n",
    "print('There are ', len(train), ' songs in the train set')\n",
    "print('There are ', len(test), ' songs in the test set')\n",
    "\n",
    "#since we read in all of the data, we can now close the files\n",
    "ftest.close()\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Top Words == Test Top Words ? :  True\n"
     ]
    }
   ],
   "source": [
    "#are train_top_words and test_top_words identical? you'd think they should be\n",
    "is_true = (train_top_words == test_top_words)\n",
    "print('Train Top Words == Test Top Words ? : ', is_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#they are they same so let's just call one of them \"top words\" and use that from now on:\n",
    "top_words = test_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29173"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_words) #this is the number of characters, let's try to split them into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few words of top words : \n",
      "%i,the,you,to,and,a,me,it,not,in,my,is,of,your,that,do,on,are,we,am,will,all,for,no,be,have,love,so,\n",
      "Should be i,the,you,to...\n"
     ]
    }
   ],
   "source": [
    "#notice that the first word has a % character in front of it.\n",
    "#remove that character:\n",
    "top_words = top_words[1:]\n",
    "print('First few words of top words : ')\n",
    "print(train_top_words[0:100])\n",
    "print('Should be i,the,you,to...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words:  ['i', 'the', 'you', 'to', 'and', 'a', 'me', 'it', 'not', 'in']\n"
     ]
    }
   ],
   "source": [
    "#split into individual words\n",
    "train_top_words = train_top_words.split(',')\n",
    "test_top_words = test_top_words.split(',')\n",
    "top_words = top_words.split(',')\n",
    "print('First 10 words: ', top_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common 100 words:  ['i', 'the', 'you', 'to', 'and', 'a', 'me', 'it', 'not', 'in', 'my', 'is', 'of', 'your', 'that', 'do', 'on', 'are', 'we', 'am', 'will', 'all', 'for', 'no', 'be', 'have', 'love', 'so', 'know', 'this', 'but', 'with', 'what', 'just', 'when', 'like', 'now', 'que', 'time', 'can', 'come', 'de', 'there', 'go', 'up', 'oh', 'la', 'one', 'they', 'out', 'down', 'get', 'she', 'was', 'see', 'if', 'got', 'never', 'from', 'he', 'feel', 'want', 'let', 'make', 'way', 'say', 'take', 'would', 'as', 'ca', 'day', 'at', 'babi', 'away', 'life', 'yeah', 'y', 'back', 'by', 'her', 'heart', 'here', 'how', 'could', 'night', 'need', 'our', 'look', 'where', 'en', 'eye', 'thing', 'world', 'more', 'caus', 'gonna', 'die', 'right', 'been', 'tell']\n"
     ]
    }
   ],
   "source": [
    "print('Most Common 100 words: ', top_words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ^^ It's strange that the word \"ca\" is in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train data song : \n",
      "TRAAAAV128F421A322,4623710,1:6,2:4,3:2,4:2,5:5,6:3,7:1,8:1,11:1,12:2,13:3,14:1,15:1,18:2,19:2,20:2,21:2,23:4,25:1,26:2,28:1,30:1,36:2,42:1,45:1,54:2,56:1,57:1,68:1,99:1,192:2,249:1,264:1,356:1,389:1,561:1,639:1,656:1,687:1,761:1,773:1,804:1,869:2,914:1,1035:1,1156:1,1221:1,1287:1,1364:1,1407:1,1533:2,1857:1,2096:1,2117:1,2482:2,2548:1,2705:1,2723:1,2868:2,2992:2,3455:1,3717:1,3851:1,4322:1,4382:1,4613:1,4713:1,4906:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# have a look at some of the song data:\n",
    "print('First train data song : ')\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we probably want to store this data in some way other than one long string for each song.\n",
    "foo = train[0]\n",
    "foo = foo.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAAAAV128F421A322'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4623710'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:6', '2:4', '3:2', '4:2', '5:5', '6:3', '7:1', '8:1', '11:1', '12:2', '13:3', '14:1', '15:1', '18:2', '19:2', '20:2', '21:2', '23:4', '25:1', '26:2', '28:1', '30:1', '36:2', '42:1', '45:1', '54:2', '56:1', '57:1', '68:1', '99:1', '192:2', '249:1', '264:1', '356:1', '389:1', '561:1', '639:1', '656:1', '687:1', '761:1', '773:1', '804:1', '869:2', '914:1', '1035:1', '1156:1', '1221:1', '1287:1', '1364:1', '1407:1', '1533:2', '1857:1', '2096:1', '2117:1', '2482:2', '2548:1', '2705:1', '2723:1', '2868:2', '2992:2', '3455:1', '3717:1', '3851:1', '4322:1', '4382:1', '4613:1', '4713:1', '4906:1\\n']\n",
      "4906:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_dict = {}\n",
    "keys_and_vals = foo[2:]\n",
    "print(keys_and_vals)\n",
    "print(keys_and_vals[-1])\n",
    "for entry in keys_and_vals:\n",
    "    entry = entry.split(':')\n",
    "    key = int(entry[0])\n",
    "    val = int(entry[1])\n",
    "    song_dict[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 6,\n",
       " 2: 4,\n",
       " 3: 2,\n",
       " 4: 2,\n",
       " 5: 5,\n",
       " 6: 3,\n",
       " 7: 1,\n",
       " 8: 1,\n",
       " 11: 1,\n",
       " 12: 2,\n",
       " 13: 3,\n",
       " 14: 1,\n",
       " 15: 1,\n",
       " 18: 2,\n",
       " 19: 2,\n",
       " 20: 2,\n",
       " 21: 2,\n",
       " 23: 4,\n",
       " 25: 1,\n",
       " 26: 2,\n",
       " 28: 1,\n",
       " 30: 1,\n",
       " 36: 2,\n",
       " 42: 1,\n",
       " 45: 1,\n",
       " 54: 2,\n",
       " 56: 1,\n",
       " 57: 1,\n",
       " 68: 1,\n",
       " 99: 1,\n",
       " 192: 2,\n",
       " 249: 1,\n",
       " 264: 1,\n",
       " 356: 1,\n",
       " 389: 1,\n",
       " 561: 1,\n",
       " 639: 1,\n",
       " 656: 1,\n",
       " 687: 1,\n",
       " 761: 1,\n",
       " 773: 1,\n",
       " 804: 1,\n",
       " 869: 2,\n",
       " 914: 1,\n",
       " 1035: 1,\n",
       " 1156: 1,\n",
       " 1221: 1,\n",
       " 1287: 1,\n",
       " 1364: 1,\n",
       " 1407: 1,\n",
       " 1533: 2,\n",
       " 1857: 1,\n",
       " 2096: 1,\n",
       " 2117: 1,\n",
       " 2482: 2,\n",
       " 2548: 1,\n",
       " 2705: 1,\n",
       " 2723: 1,\n",
       " 2868: 2,\n",
       " 2992: 2,\n",
       " 3455: 1,\n",
       " 3717: 1,\n",
       " 3851: 1,\n",
       " 4322: 1,\n",
       " 4382: 1,\n",
       " 4613: 1,\n",
       " 4713: 1,\n",
       " 4906: 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 656, 2705, 18, 19, 20, 21, 23, 25, 26, 28, 30, 389, 2992, 1156, 2723, 804, 3717, 42, 1287, 4613, 45, 687, 2096, 561, 2482, 773, 4382, 54, 56, 57, 4906, 192, 1857, 1035, 68, 1221, 3851, 3455, 1364, 761, 36, 2117, 4322, 99, 356, 869, 4713, 914, 2548, 2868, 249, 1407, 1533, 264, 639])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "song_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([6, 4, 2, 2, 5, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 2, 2, 2, 4, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TO DO\n",
    "### Above I converted one song's data into a dictionary, but we should do this for each song in the test and train datasets.\n",
    "### Maybe make a Pandas DataFrame with columns : 'ID1','ID2','SongDict' ?\n",
    "### Then, sum all the counts and plot rank vs. frequency to see if it follows a power law\n",
    "### Could also use the total counts to make a word cloud where words are sized by their frequency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
